/*
 * GStreamer gstreamer-fastsamtensordecoder
 * Copyright (C) 2023 Collabora Ltd.
 *
 * gstfastsamtensordecoder.c
 *
 * This library is free software; you can redistribute it and/or
 * modify it under the terms of the GNU Library General Public
 * License as published by the Free Software Foundation; either
 * version 2 of the License, or (at your option) any later version.
 *
 * This library is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
 * Library General Public License for more details.
 *
 * You should have received a copy of the GNU Library General Public
 * License along with this library; if not, write to the
 * Free Software Foundation, Inc., 51 Franklin St, Fifth Floor,
 * Boston, MA 02110-1301, USA.
 */

/**
 * SECTION:element-samtensordecoder.c
 * @short_description: Decode tensors from a FastSAM detection and segmentation
 * neural network.
 *
 *
 * This element can parse per-buffer inference tensor meta data generated by an upstream
 * inference element
 *
 *
 * ## Example launch command:
 *
 * note: image resolution may need to be adapted to the model, if the model expects
 * a certain input resolution. The `videoscale` element in the pipeline below will scale
 * the image, using padding if required, to 1024x1024
 *
 *
 * GST_DEBUG=fastsamtensordecoder
 * gst-launch-1.0 multifilesrc location=strawberry_crops.jpg ! jpegdec ! \
 * videoconvertscale ! "video/x-raw, format=RGBA, width=1024" ! \
 * videobox bottom=-124 ! onnxinference execution-provider=cpu \
 * model-file=segmentation.onnx input-image-format=chw input-image-datatype=4 !\
 * fastsamtensordecoder ! fakevideosink
 *
 */

#ifdef HAVE_CONFI_H
#include "config.h"
#endif

#include "gstfastsamtensordecoder.h"

#include <gst/gst.h>
#include <gst/video/video.h>

#include <gst/analytics/gstanalyticsmeta.h>
#include <gst/analytics/gstanalyticsobjectdetectionmtd.h>

#include "tensor/gsttensormeta.h"

#define GST_FASTSAM_TENSOR_DECODER_TRACE_CANDIDATES 1

#define GST_MODEL_FASTSAM_SEGMENTATION_MASK \
  "Gst.Model.FastSAM.Segmentation.Masks"
#define GST_MODEL_FASTSAM_SEGMENTATION_LOGITS \
  "Gst.Model.FastSAM.Segmentation.Logits"

GST_DEBUG_CATEGORY_STATIC (fastsam_tensor_decoder_debug);
#define GST_CAT_DEFAULT fastsam_tensor_decoder_debug

GST_ELEMENT_REGISTER_DEFINE (fastsam_tensor_decoder, "fastsamtensordecoder",
    GST_RANK_PRIMARY, GST_TYPE_FASTSAM_TENSOR_DECODER);

/* GstFastSAMTensorDecoder properties, see properties description in
 * gst_fastsam_tensor_decoder_class_init for more details. */
enum
{
  PROP_0,
  PROP_BOX_CONFI_THRESH,
  PROP_CLS_CONFI_THRESH,
  PROP_IOU_THRESH,
  PROP_MAX_DETECTION,
  PROP_MASK_TENSOR_NAME,
  PROP_LOGITS_TENSOR_NAME
};

#if GST_FASTSAM_TENSOR_DECODER_TRACE_CANDIDATES==1
/* For debug purpose */
typedef struct _DebugCandidates
{
  GstFastSAMTensorDecoder *self;
  gsize fields;                 /* Fields count do debug */
  gsize offset;                 /* Fields offset */
  gsize start;                  /* First field index to debug */
} DebugCandidates;
#endif

typedef struct _GstFastSAMTensorDecoderPrivate
{
  /* Candidates with a class confidence level above threshold. */
  GPtrArray *sel_candidates;

  /* Final candidates selected that respect class confidence level,
   * NMS and maximum detection. */
  GPtrArray *selected;

  /* Analytics-meta initialization parameters. */
  GstAnalyticsRelationMetaInitParams rmeta_init_params;

  /* Tensor-id identifying mask tensors out of FastSAM inference process. */
  GQuark mask_tensor_id;

  /* Tensor-id identifying logits tensors out of FastSAM inference process. */
  GQuark logits_tensor_id;
} GstFastSAMTensorDecoderPrivate;


typedef struct _BBox
{
  guint x;
  guint y;
  guint w;
  guint h;
} BBox;

/* Convenient macro to retrieve private members */
#define PRIV(s) gst_fastsam_tensor_decoder_get_instance_private ( \
    GST_FASTSAM_TENSOR_DECODER (s))

/* Default properties value */
static const float DEFAULT_BOX_CONFI_THRESH = 0.4f;
static const float DEFAULT_CLS_CONFI_THRESH = 0.4f;
static const float DEFAULT_IOU_THRESH = 0.7f;
static const gsize DEFAULT_MAX_DETECTION = 100;

/* Global variable storing class for OD. Generally OD has class
 * and we need to provide one but this class is just a placeholder.*/
GQuark OOI_CLASS_ID;

/* To tensor-id are defined by a string that is converted to quark
 * which is just an integer value using a hash function. For efficiency
 * we compare on the quark (hash value). Since tensor-id never change we
 * just calculate the hash once during initialization and store the value in
 * these variables. */
GQuark GST_MODEL_FASTSAM_SEGMENTATION_MASKS_ID;
GQuark GST_MODEL_FASTSAM_SEGMENTATION_LOGITS_ID;

/* GStreamer element srcpad template. Template of a srcpad that can receive
 * any raw video. */
static GstStaticPadTemplate gst_fastsam_tensor_decoder_src_template =
GST_STATIC_PAD_TEMPLATE ("src",
    GST_PAD_SRC,
    GST_PAD_ALWAYS,
    GST_STATIC_CAPS ("video/x-raw"));

/* GStreamer element sinkpad template. Template of a sinkpad that can receive
 * any raw video. */
static GstStaticPadTemplate gst_fastsam_tensor_decoder_sink_template =
GST_STATIC_PAD_TEMPLATE ("sink",
    GST_PAD_SINK,
    GST_PAD_ALWAYS,
    GST_STATIC_CAPS ("video/x-raw"));

/* Prototypes */
static void gst_fastsam_tensor_decoder_set_property (GObject * object,
    guint prop_id, const GValue * value, GParamSpec * pspec);
static void gst_fastsam_tensor_decoder_get_property (GObject * object,
    guint prop_id, GValue * value, GParamSpec * pspec);

static void gst_fastsam_tensor_decoder_finalize (GObject * object);

static GstFlowReturn gst_fastsam_tensor_decoder_transform_ip (GstBaseTransform *
    trans, GstBuffer * buf);
static gboolean gst_fastsam_tensor_decoder_set_caps (GstBaseTransform * trans,
    GstCaps * incaps, GstCaps * outcaps);
static void gst_fastsam_tensor_decoder_decode_masks_f32 (GstFastSAMTensorDecoder
    * self, GstBuffer * buf, GstTensor * masks_tensor,
    GstAnalyticsRelationMeta * rmeta);
static void gst_fastsam_tensor_decoder_decode_logits (GstFastSAMTensorDecoder *
    self, GstBuffer * buf, GstTensor * masks_tensor,
    GstAnalyticsRelationMeta * rmeta);

/* Define a GstElement that has private member and based on GstBaseTransform. */
G_DEFINE_TYPE_WITH_PRIVATE (GstFastSAMTensorDecoder, gst_fastsam_tensor_decoder,
    GST_TYPE_BASE_TRANSFORM);

static void
gst_fastsam_tensor_decoder_class_init (GstFastSAMTensorDecoderClass * klass)
{
  GObjectClass *gobject_class = (GObjectClass *) klass;
  GstElementClass *element_class = (GstElementClass *) klass;
  GstBaseTransformClass *basetransform_class = (GstBaseTransformClass *) klass;

  /* Define GstFastSAMTensorDecoder debug category. */
  GST_DEBUG_CATEGORY_INIT (fastsam_tensor_decoder_debug, "fastsamtensordecoder",
      0, "Tensor decoder for FastSAM segmentation N.N.");

  /* Set GObject vmethod to get and set property */
  gobject_class->set_property = gst_fastsam_tensor_decoder_set_property;
  gobject_class->get_property = gst_fastsam_tensor_decoder_get_property;

  /* Set GObject vmethod finalize */
  gobject_class->finalize = gst_fastsam_tensor_decoder_finalize;

  /* Define GstFastSAMTensorDecoder properties using GObject properties
   * interface.*/
  g_object_class_install_property (G_OBJECT_CLASS (klass),
      PROP_BOX_CONFI_THRESH,
      g_param_spec_float ("box-confidence-threshold",
          "Box location confidence threshold",
          "Boxes with a location confidence level inferior to this threshold "
          "will be excluded",
          0.0, 1.0, DEFAULT_BOX_CONFI_THRESH,
          (GParamFlags) (G_PARAM_READWRITE | G_PARAM_STATIC_STRINGS)));

  g_object_class_install_property (G_OBJECT_CLASS (klass),
      PROP_CLS_CONFI_THRESH,
      g_param_spec_float ("class-confidence-threshold",
          "Class confidence threshold",
          "Classes with a confidence level inferior to this threshold "
          "will be excluded",
          0.0, 1.0, DEFAULT_CLS_CONFI_THRESH,
          (GParamFlags) (G_PARAM_READWRITE | G_PARAM_STATIC_STRINGS)));

  g_object_class_install_property (G_OBJECT_CLASS (klass),
      PROP_IOU_THRESH,
      g_param_spec_float ("iou-threshold",
          "Maximum IOU threshold",
          "Maximum intersection-over-union between bounding boxes to "
          "consider them distinct.",
          0.0, 1.0, DEFAULT_IOU_THRESH,
          (GParamFlags) (G_PARAM_READWRITE | G_PARAM_STATIC_STRINGS)));

  g_object_class_install_property (G_OBJECT_CLASS (klass),
      PROP_MAX_DETECTION,
      g_param_spec_uint ("max-detections",
          "Maximum object/masks detections.",
          "Maximum object/masks detections.",
          0, G_MAXUINT, DEFAULT_MAX_DETECTION,
          (GParamFlags) (G_PARAM_READWRITE | G_PARAM_STATIC_STRINGS)));

  g_object_class_install_property (G_OBJECT_CLASS (klass),
      PROP_MASK_TENSOR_NAME,
      g_param_spec_string ("tensors-name-masks",
          "Mask tensors name",
          "Name that identify FastSAM mask tensors.",
          GST_MODEL_FASTSAM_SEGMENTATION_MASK,
          (GParamFlags) (G_PARAM_READWRITE | G_PARAM_CONSTRUCT |
              G_PARAM_STATIC_STRINGS)));

  g_object_class_install_property (G_OBJECT_CLASS (klass),
      PROP_LOGITS_TENSOR_NAME,
      g_param_spec_string ("tensors-name-logits",
          "Logits tensors name",
          "Name that identify FastSAM logits tensors.",
          GST_MODEL_FASTSAM_SEGMENTATION_LOGITS,
          (GParamFlags) (G_PARAM_READWRITE | G_PARAM_CONSTRUCT |
              G_PARAM_STATIC_STRINGS)));

  /* Element description. */
  gst_element_class_set_static_metadata (element_class, "fastsamtensordecoder",
      "Filter/Effect/Video",
      "Decode tensors output from the inference of FastSAM model (segmentation)"
      " on video frames. The original repository of the FastSAM is located at"
      " https://github.com/CASIA-IVA-Lab/FastSAM. For easy experimentation a"
      " strawberry segmentation model based on FastSAM architecture in Onnx "
      " format can be found at https://col.la/gstonnxmodelseg . This model "
      "already has tensors name embedded matching default "
      "values of tensors-masks-name and tensors-logits-name properties. It's "
      "also possible to embed tensor-ids into any model based on FastSAM "
      "architecture to allow this tensor-decoder to decode tensors. This "
      "process is described in the Readme of this repository: "
      "https://col.la/gstonnxmodels",
      "Daniel Morin <daniel.morin@collabora.com>");

  /* Add pads to element base on pad template defined earlier */
  gst_element_class_add_pad_template (element_class,
      gst_static_pad_template_get (&gst_fastsam_tensor_decoder_src_template));
  gst_element_class_add_pad_template (element_class,
      gst_static_pad_template_get (&gst_fastsam_tensor_decoder_sink_template));

  /* Set GstBaseTransform vmethod transform_ip. This methode is called
   * by the srcpad when it receive buffer. ip stand for in-place meaning the
   * buffer remain unchanged by the element. Tensor-decoder only monitor
   * buffer it receive for a meta attach to the buffer that is a GstTensorMeta
   * and has a tensor-id can be handled by GstFastSAMTensorDecoder. */
  basetransform_class->transform_ip =
      GST_DEBUG_FUNCPTR (gst_fastsam_tensor_decoder_transform_ip);

  /* Set GstBaseTransform set_caps vmethod. This will be called once the
   * capability negotiation has been completed. We will be able to extract
   * resolution from this callback. */
  basetransform_class->set_caps =
      GST_DEBUG_FUNCPTR (gst_fastsam_tensor_decoder_set_caps);

  /* Calculate the class id placeholder (also a quark) that will be set on all
   * OD analytics-meta. */
  OOI_CLASS_ID = g_quark_from_static_string ("FastSAM-None");

  /* Calculate the FastSAM Mask tensor-id */
  GST_MODEL_FASTSAM_SEGMENTATION_MASKS_ID =
      g_quark_from_static_string (GST_MODEL_FASTSAM_SEGMENTATION_MASK);

  /* Calculate the FastSAM Logits tensor-id */
  GST_MODEL_FASTSAM_SEGMENTATION_LOGITS_ID =
      g_quark_from_static_string (GST_MODEL_FASTSAM_SEGMENTATION_LOGITS);
}

static void
gst_fastsam_tensor_decoder_init (GstFastSAMTensorDecoder * self)
{
  /* GstFastSAMTensorDecoder instance initialization */
  GstFastSAMTensorDecoderPrivate *priv = PRIV (self);
  self->box_confi_thresh = DEFAULT_BOX_CONFI_THRESH;
  self->cls_confi_thresh = DEFAULT_CLS_CONFI_THRESH;
  self->iou_thresh = DEFAULT_IOU_THRESH;
  self->max_detection = DEFAULT_MAX_DETECTION;
  priv->sel_candidates = NULL;
  priv->selected = NULL;
  priv->rmeta_init_params.initial_buf_size = 1024;
  priv->rmeta_init_params.initial_relation_order = 10;
  gst_base_transform_set_passthrough (GST_BASE_TRANSFORM (self), FALSE);
}

static void
gst_fastsam_tensor_decoder_finalize (GObject * object)
{
  GstFastSAMTensorDecoder *self = GST_FASTSAM_TENSOR_DECODER (object);
  GstFastSAMTensorDecoderPrivate *priv = PRIV (self);

  if (priv->sel_candidates) {
    g_ptr_array_unref (g_steal_pointer (&priv->sel_candidates));
  }

  if (priv->selected) {
    g_ptr_array_unref (g_steal_pointer (&priv->selected));
  }

  G_OBJECT_CLASS (gst_fastsam_tensor_decoder_parent_class)->finalize (object);
}

static void
gst_fastsam_tensor_decoder_set_property (GObject * object, guint prop_id,
    const GValue * value, GParamSpec * pspec)
{
  GstFastSAMTensorDecoder *self = GST_FASTSAM_TENSOR_DECODER (object);
  GstFastSAMTensorDecoderPrivate *priv;

  switch (prop_id) {
    case PROP_BOX_CONFI_THRESH:
      GST_OBJECT_LOCK (self);
      self->box_confi_thresh = g_value_get_float (value);
      GST_OBJECT_UNLOCK (self);
      break;
    case PROP_CLS_CONFI_THRESH:
      GST_OBJECT_LOCK (self);
      self->cls_confi_thresh = g_value_get_float (value);
      GST_OBJECT_UNLOCK (self);
      break;
    case PROP_IOU_THRESH:
      GST_OBJECT_LOCK (self);
      self->iou_thresh = g_value_get_float (value);
      GST_OBJECT_UNLOCK (self);
      break;
    case PROP_MAX_DETECTION:
      GST_OBJECT_LOCK (self);
      self->max_detection = g_value_get_uint (value);
      GST_OBJECT_UNLOCK (self);
      break;
    case PROP_MASK_TENSOR_NAME:
      GST_OBJECT_LOCK (self);
      priv = PRIV (self);
      priv->mask_tensor_id = g_quark_from_string (g_value_get_string (value));
      GST_OBJECT_UNLOCK (self);
      break;
    case PROP_LOGITS_TENSOR_NAME:
      GST_OBJECT_LOCK (self);
      priv = PRIV (self);
      priv->logits_tensor_id = g_quark_from_string (g_value_get_string (value));
      GST_OBJECT_UNLOCK (self);
      break;
    default:
      G_OBJECT_WARN_INVALID_PROPERTY_ID (object, prop_id, pspec);
      break;
  }
}

static void
gst_fastsam_tensor_decoder_get_property (GObject * object, guint prop_id,
    GValue * value, GParamSpec * pspec)
{
  GstFastSAMTensorDecoder *self = GST_FASTSAM_TENSOR_DECODER (object);
  GstFastSAMTensorDecoderPrivate *priv;

  switch (prop_id) {
    case PROP_BOX_CONFI_THRESH:
      g_value_set_float (value, self->box_confi_thresh);
      break;
    case PROP_CLS_CONFI_THRESH:
      g_value_set_float (value, self->cls_confi_thresh);
      break;
    case PROP_IOU_THRESH:
      g_value_set_float (value, self->iou_thresh);
      break;
    case PROP_MAX_DETECTION:
      g_value_set_uint (value, self->max_detection);
      break;
    case PROP_MASK_TENSOR_NAME:
      GST_OBJECT_LOCK (self);
      priv = PRIV (self);
      g_value_set_string (value, g_quark_to_string (priv->mask_tensor_id));
      GST_OBJECT_UNLOCK (self);
      break;
    case PROP_LOGITS_TENSOR_NAME:
      GST_OBJECT_LOCK (self);
      priv = PRIV (self);
      g_value_set_string (value, g_quark_to_string (priv->logits_tensor_id));
      GST_OBJECT_UNLOCK (self);
      break;
    default:
      G_OBJECT_WARN_INVALID_PROPERTY_ID (object, prop_id, pspec);
      break;
  }
}

/* gst_fastsam_tensor_decoder_get_tensor_meta
 * @buf:in: buffer
 * @mask_tensor:out: Mask tensor
 * @logits_tensor:out: Logits tensor
 * @return: TRUE if buf has mask and logits tensor attach to it.
 * Retrieve FastSAM masks and logits tensors from buffer.
 */
static gboolean
gst_fastsam_tensor_decoder_get_tensor_meta (GstFastSAMTensorDecoder * self,
    GstBuffer * buf, GstTensor ** mask_tensor, GstTensor ** logits_tensor)
{
  GList *tensor_metas;
  GList *iter;
  gsize mask_tensor_idx, logits_tensor_idx;

  g_return_val_if_fail (mask_tensor != NULL, FALSE);
  g_return_val_if_fail (logits_tensor != NULL, FALSE);
  *mask_tensor = NULL;
  *logits_tensor = NULL;

  /* Retrieve all TensorMeta attach the buffer */
  tensor_metas = gst_tensor_meta_get_all_from_buffer (buf);
  if (!tensor_metas) {
    GST_TRACE_OBJECT (self, "No tensor meta");
    goto cleanup;
  }


  /* Iterate through all to buffer and verify if one has a tensor-id
   * that match the tensor-id that GstFastSAMTensorDecoder can handle */
  for (iter = tensor_metas; iter != NULL; iter = g_list_next (iter)) {
    GstTensorMeta *tensor_meta = (GstTensorMeta *) iter->data;

    GST_TRACE_OBJECT (self, "Num tensors %d", tensor_meta->num_tensors);

    /* Retrieve the index of the tensor that has a tensor-id matching
     * GST_MODEL_FASTSAM_SEGMENTATION_MASKS_ID in the GstTensorMeta. */
    mask_tensor_idx = gst_tensor_meta_get_index_from_id (tensor_meta,
        GST_MODEL_FASTSAM_SEGMENTATION_MASKS_ID);

    /* Retrieve the index of the tensor that has a tensor-id matching
     * GST_MODEL_FASTSAM_SEGMENTATION_LOGITS_ID in the GstTensorMeta. */
    logits_tensor_idx = gst_tensor_meta_get_index_from_id (tensor_meta,
        GST_MODEL_FASTSAM_SEGMENTATION_LOGITS_ID);

    if (mask_tensor_idx != GST_TENSOR_MISSING_ID
        && logits_tensor_idx != GST_TENSOR_MISSING_ID) {
      GST_TRACE_OBJECT (self, "Masks tensor id: %lu", mask_tensor_idx);
      GST_TRACE_OBJECT (self, "Masks tensor id: %lu", logits_tensor_idx);

      *mask_tensor = &tensor_meta->tensor[mask_tensor_idx];
      *logits_tensor = &tensor_meta->tensor[logits_tensor_idx];
      break;
    }
  }

cleanup:
  g_list_free (tensor_metas);

  return *mask_tensor != NULL && *logits_tensor != NULL;
}

/* gst_fastsam_tensor_decoder_set_caps:
 *
 * Callback on caps negociation completed. We use it here to retrieve
 * video resolution. See GstBaseTransform for more details.
 */
static gboolean
gst_fastsam_tensor_decoder_set_caps (GstBaseTransform * trans, GstCaps * incaps,
    GstCaps * outcaps)
{
  GstFastSAMTensorDecoder *self = GST_FASTSAM_TENSOR_DECODER (trans);

  if (!gst_video_info_from_caps (&self->video_info, incaps)) {
    GST_ERROR_OBJECT (self, "Failed to parse caps");
    return FALSE;
  }

  if (gst_base_transform_is_passthrough (trans)) {
    GST_ERROR_OBJECT (self, "Failed. Can't handle passthrough");
    return FALSE;
  }

  return TRUE;
}

/* gst_fastsam_tensor_decoder_transform_ip:
 * @trans: Instance
 * @buf:inout: Buffer containing media and where tensors can be attached
 * @return: Flow errors
 * Decode FastSAM tensors, post-process tensors and store decoded information
 * into an analytics-meta that is attached to the buffer before been pushed
 * downstream.
 */
static GstFlowReturn
gst_fastsam_tensor_decoder_transform_ip (GstBaseTransform * trans,
    GstBuffer * buf)
{
  GstFastSAMTensorDecoder *self = GST_FASTSAM_TENSOR_DECODER (trans);
  GstFastSAMTensorDecoderPrivate *priv = PRIV (self);
  GstTensor *masks_tensor, *logits_tensor;
  GstAnalyticsRelationMeta *rmeta;

  if (gst_fastsam_tensor_decoder_get_tensor_meta (self, buf, &masks_tensor,
          &logits_tensor)) {

    /* Retrieve or attach an analytics-relation-meta to the buffer.
     * Analytics-relation-meta are container that can reveive multiple
     * analytics-meta, like OD and Segmentation. The following call will only
     * retrieve an analytics-relation-meta if it exist or create one if it
     * does not exist. */
    rmeta = gst_buffer_add_analytics_relation_meta_full (buf,
        &priv->rmeta_init_params);
    g_return_val_if_fail (rmeta != NULL, GST_FLOW_ERROR);

    /* Decode masks_tensor and attach the information in a structured way
     * to rmeta.
     * TODO: I think we need to send both tensors masks and logits
     * to gst_fastsam_tensor_decoder_decode_masks_f32 since both are
     * required simultanously to extract the segmentation. If this is the case
     * we probably should rename gst_fastsam_tensor_decoder_decode_masks_f32 to
     * gst_fastsam_tensor_decoder_decode_f32. */
    gst_fastsam_tensor_decoder_decode_masks_f32 (self, buf, masks_tensor,
        rmeta);

    /* Decode logits_tensor and attach the information in a structured way
     * to rmeta.*/
    gst_fastsam_tensor_decoder_decode_logits (self, buf, logits_tensor, rmeta);
  }

  return GST_FLOW_OK;
}

/* Evaluate if there's an intersection between segement s1 and s2 */
static guint
linear_intersection (guint s1_min, guint s1_max, guint s2_min, guint s2_max)
{
  guint tmp;
  if (s1_max > s2_min && s2_max > s1_min) {
    if (s1_min > s2_min) {
      tmp = (s2_max > s1_max) ? s1_max : s2_max;
      return tmp - s1_min;
    } else {
      tmp = (s1_max > s2_max) ? s2_max : s1_max;
      return tmp - s2_min;
    }
  }
  return 0.0f;
}

static gfloat
iou (guint bb1_x, guint bb1_y, guint bb1_w, guint bb1_h,
    guint bb2_x, guint bb2_y, guint bb2_w, guint bb2_h)
{
  /* Rational: linear intersection is much faster to calculate then
   * 2d intersection. We project the two bounding boxes considered for
   * intersection on one axis and verify if the segments the create intersect.
   * If they don't, the bounding boxes can't intersect in 2d and we don't
   * need to verify if they intersect on the other dimension. If they
   * intersect on the first dimension we verify if they intersec on the other
   * dimension. Again if the don't intersect the bounding boxes can't intersect
   * on in a 2D space. If they intersected on both axis we calculate the IoU.*/
  const guint x_intersection =
      linear_intersection (bb1_x, bb1_x + bb1_w, bb2_x, bb2_x + bb2_w);
  if (x_intersection > 0) {
    const guint y_intersection = linear_intersection (bb1_y, bb1_y + bb1_h,
        bb2_y, bb2_y + bb2_h);
    if (y_intersection > 0) {
      const guint bb1_area = bb1_w * bb1_h;
      const guint bb2_area = bb2_w * bb2_h;
      const guint intersect_area = x_intersection * y_intersection;
      const guint union_area = bb1_area + bb2_area - intersect_area;
      return union_area == 0 ? 0.0f : ((gfloat) intersect_area) / union_area;
    }
  }

  return 0.0f;
}

/* Extract bounding box from tensor data */
static void
gst_fastsam_tensor_decoder_convert_bbox (gfloat * candidate, gsize * offset,
    BBox * bbox)
{
  gfloat w = *(candidate + offset[2]);
  gfloat h = *(candidate + offset[3]);
  bbox->x = *(candidate + offset[0]) - (w / 2);
  bbox->y = *(candidate + offset[1]) - (h / 2);
  bbox->w = w + 0.5;
  bbox->h = h + 0.5;
}

/* Calculate iou between boundingbox of candidate c1 and c2
 */
static gfloat
gst_fastsam_tensor_decoder_iou (gfloat * c1, gfloat * c2, gsize * offset,
    BBox * bb1, BBox * bb2)
{
  gst_fastsam_tensor_decoder_convert_bbox (c1, offset, bb1);
  gst_fastsam_tensor_decoder_convert_bbox (c2, offset, bb2);
  return iou (bb1->x, bb1->y, bb1->w, bb1->h, bb2->x, bb2->y, bb2->w, bb2->h);
}

/* Compare c1 and c2
 * Utility function for sorting candiates based on the a field identified
 * by offset.
 */
static gint
gst_fastsam_tensor_decoder_sort_candidates (gconstpointer c1, gconstpointer c2,
    gpointer offset)
{
  const gfloat *c1_confi =
      (*((const gfloat **) c1) + GPOINTER_TO_SIZE (offset));
  const gfloat *c2_confi =
      (*((const gfloat **) c2) + GPOINTER_TO_SIZE (offset));
  return *c1_confi < *c2_confi ? 1 : *c1_confi > *c2_confi ? -1 : 0;
}

#if GST_FASTSAM_TENSOR_DECODER_TRACE_CANDIDATES==1
static void
gst_fastsam_tensor_decoder_debug_print_candidate (gpointer candidate_,
    gpointer data)
{
  DebugCandidates *ctx = data;
  const gfloat *candidate = candidate_;

  for (gsize i = ctx->start; i < ctx->fields + ctx->start; i++) {
    GST_TRACE_OBJECT (ctx->self, "Field %lu: %f", i,
        *(candidate + (i * ctx->offset)));
  }
}
#endif

static void
gst_fastsam_tensor_decoder_decode_masks_f32 (GstFastSAMTensorDecoder * self,
    GstBuffer * buf, GstTensor * masks_tensor, GstAnalyticsRelationMeta * rmeta)
{
  GstFastSAMTensorDecoderPrivate *priv = PRIV (self);
  /*guint batch_size = masks_tensor->dims[0]; */
  /*guint num_masks = masks_tensor->dims[1]; */
  GstMemory *memory = NULL;
  GstMapInfo map_info;
  gfloat *data, *candidate, **candidates, iou;
  gboolean rv;
  gsize offset, x_offset, y_offset, w_offset, h_offset, c_offset, offsets[4];
  GPtrArray *sel_candidates = priv->sel_candidates;
  GPtrArray *selected = priv->selected;
  gboolean keep;
  BBox bb1, bb2;
  GstAnalyticsODMtd od_mtd;

  /* Retrieve memory at index 0 and map it in READ mode */
  memory = gst_buffer_peek_memory (masks_tensor->data, 0);
  g_return_if_fail (memory != NULL);
  rv = gst_memory_map (memory, &map_info, GST_MAP_READ);
  g_return_if_fail (rv);
  data = (gfloat *) map_info.data;
  GST_TRACE_OBJECT (self, "Mask Tensor shape dims %d", masks_tensor->num_dims);

  /* Trace masks tensor dimensions */
  for (gint i = 0; i < masks_tensor->num_dims; i++) {
    GST_TRACE_OBJECT (self, "Masks Tensor dim %d: %ld", i,
        masks_tensor->dims[i]);
  }

  /* Allocated array to store selected candidates */
  if (sel_candidates == NULL) {
    /* Number of candidates can be large, keep the array to avoid frequent
     * allocation */
    sel_candidates = g_ptr_array_new_full (masks_tensor->dims[2], NULL);
    priv->sel_candidates = sel_candidates;
    selected = g_ptr_array_new_full (masks_tensor->dims[2], NULL);
    priv->selected = selected;
  } else {
    /* Reset lengths when we re-use arrays */
    sel_candidates->len = 0;
    selected->len = 0;
  }

  /* masks_tensor->dims[2] contain the number of candidates. Let's call the
   * number of candidates C. We store this value in offset as we use it
   * calculate the offset of candidate fields. The variable data above point
   * at the masks tensor data, but candidates data is organize like a plane.
   * Candidates bbox X coord fields from 0 to C start at the begining of the
   * tensor data and are continguous in memory, followed by all candidates
   * field Y, followed by field W, ... followed by field class confidence level,
   * ..., followed by all candidates mask0, ..., followed by all candidates
   * mask31. Bellow we pre-calculate each field offset relative to the
   * candidate pointer (pointer to field X), which will allow us to easily
   * access each candiates field.
   * */
  offset = masks_tensor->dims[2];
  x_offset = 0;
  y_offset = offset;
  w_offset = 2 * offset;
  h_offset = 3 * offset;
  c_offset = 4 * offset;
  offsets[0] = x_offset;
  offsets[1] = y_offset;
  offsets[2] = w_offset;
  offsets[3] = h_offset;

  candidate = data;
  for (gsize c_idx = 0; c_idx < masks_tensor->dims[2]; c_idx++) {
    /* FastSAM only has one class, but this confidence level is still used
     * to evaluate the relevance of the candidate. Here we filter candidates
     * based on their class confidence level.*/
    if (candidate[c_offset] > self->cls_confi_thresh) {
      g_ptr_array_add (sel_candidates, candidate);

      /*
         GST_TRACE_OBJECT (self,
         "%lu: x,y=(%f,%f) w,h=(%f,%f) c=%f",
         c_idx,
         candidate[x_offset],
         candidate[y_offset],
         candidate[w_offset],
         candidate[h_offset],
         candidate[c_offset]);
       */
    }

    /* Pointer arithmetic, going to the next candidate. This is the candidate
     * pointer that is now incremented to the next candidate which is also
     * the field X of the next candidate.*/
    candidate += 1;
  }

  GST_TRACE_OBJECT (self, "Selected candidates count: %u", sel_candidates->len);

  /* We the remaining candidates because, in the next selection phase we have
   * a maximum and we want to make sure that considered only the candidates
   * with the highest class confidence level before potentially reaching the
   * maximum.*/
  g_ptr_array_sort_with_data (sel_candidates,
      gst_fastsam_tensor_decoder_sort_candidates, GSIZE_TO_POINTER (c_offset));

#if GST_FASTSAM_TENSOR_DECODER_TRACE_CANDIDATES==1
  /* For debug purpose only. Prints candidates before NMS */
  DebugCandidates ctx;
  ctx.start = 0;
  ctx.fields = 5;
  ctx.offset = offset;
  ctx.self = self;
  g_ptr_array_foreach (sel_candidates,
      gst_fastsam_tensor_decoder_debug_print_candidate, (gpointer) & ctx);
#endif

  /* Algorithm in part inspired by OpenCV NMSBoxes */
  candidates = (gfloat **) sel_candidates->pdata;
  for (gsize c = 0; c < sel_candidates->len; c++) {
    keep = TRUE;

    /* We only want to a NMS using IoU between candidates we've decided to
     * keep and the new one we considering to keep. selected array contain
     * the candidates we decided to keep and candidates[c] is the candidate
     * we're considering to keep or reject */
    for (gsize s = 0; s < selected->len && keep; s++) {
      iou = gst_fastsam_tensor_decoder_iou (candidates[c], selected->pdata[s],
          offsets, &bb1, &bb2);
      keep = iou <= self->iou_thresh;
    }

    if (keep) {
      g_ptr_array_add (selected, sel_candidates->pdata[c]);

      /* We add the analytics-objectdetection-meta to the buffer. Since
       * there's only one class the class confidence level is set to -1.0
       * as it's deemed not important. */
      gst_analytics_relation_meta_add_od_mtd (rmeta, OOI_CLASS_ID,
          bb1.x, bb1.y, bb1.w, bb1.h, -1.0, &od_mtd);

      /* If the maximum number of candidate selected is reached exit the
       * selection process. */
      if (selected->len >= self->max_detection) {
        break;
      }
    }
  }

  GST_TRACE_OBJECT (self, "Selected count: %u", selected->len);

#if GST_FASTSAM_TENSOR_DECODER_TRACE_CANDIDATES==1
  /* For debug purpose only. Prints candidates after NMS */
  ctx.start = 0;
  ctx.fields = 5;
  ctx.offset = offset;
  ctx.self = self;
  g_ptr_array_foreach (selected,
      gst_fastsam_tensor_decoder_debug_print_candidate, (gpointer) & ctx);
#endif

  /* We unmap the memory */
  gst_memory_unmap (memory, &map_info);
}

static void
gst_fastsam_tensor_decoder_decode_logits (GstFastSAMTensorDecoder * self,
    GstBuffer * buf, GstTensor * logits_tensor,
    GstAnalyticsRelationMeta * rmeta)
{

  GST_TRACE_OBJECT (self, "Logits Tensor shape dims %d",
      logits_tensor->num_dims);
  for (gint i = 0; i < logits_tensor->num_dims; i++) {
    GST_TRACE_OBJECT (self, "Logits Tensor dim %d: %ld", i,
        logits_tensor->dims[i]);
  }
}
